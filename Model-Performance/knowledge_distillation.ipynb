{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üìò Knowledge Distillation Assignment\n",
    "# Task: Reduce model latency using distillation\n",
    "# Dataset: CIFAR-10\n",
    "# Teacher: ResNet-50\n",
    "# Student: ResNet-18\n",
    "# =========================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# 1Ô∏è‚É£ Load CIFAR-10\n",
    "# ----------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2Ô∏è‚É£ Define Teacher and Student\n",
    "# ----------------------------\n",
    "teacher = models.resnet50(pretrained=False, num_classes=10).to(device)\n",
    "student = models.resnet18(pretrained=False, num_classes=10).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923da4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3Ô∏è‚É£ Define Training Utilities\n",
    "# ----------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return 100. * correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8328ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4Ô∏è‚É£ Distillation Loss\n",
    "# ----------------------------\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, T=4.0, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.alpha = alpha\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, true_labels):\n",
    "        # Hard-label loss\n",
    "        hard_loss = F.cross_entropy(student_logits, true_labels)\n",
    "        # Soft-label loss\n",
    "        soft_teacher = F.log_softmax(teacher_logits / self.T, dim=1)\n",
    "        soft_student = F.log_softmax(student_logits / self.T, dim=1)\n",
    "        soft_loss = self.kl(soft_student, soft_teacher) * (self.T ** 2)\n",
    "        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ff257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5Ô∏è‚É£ Train Teacher\n",
    "# ----------------------------\n",
    "print(\"Training Teacher Model (ResNet-50)...\")\n",
    "opt_t = optim.SGD(teacher.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(2):  # keep short for demo\n",
    "    loss = train_one_epoch(teacher, trainloader, opt_t, nn.CrossEntropyLoss())\n",
    "    acc = evaluate(teacher, testloader)\n",
    "    print(f\"Epoch {epoch+1}: loss={loss:.3f}, acc={acc:.2f}%\")\n",
    "\n",
    "torch.save(teacher.state_dict(), \"teacher.pth\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6Ô∏è‚É£ Train Student with KD\n",
    "# ----------------------------\n",
    "print(\"Training Student Model (ResNet-18) with Distillation...\")\n",
    "teacher.eval()\n",
    "criterion_kd = DistillationLoss(T=4.0, alpha=0.5)\n",
    "opt_s = optim.Adam(student.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2):\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        opt_s.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            t_logits = teacher(inputs)\n",
    "        s_logits = student(inputs)\n",
    "        loss = criterion_kd(s_logits, t_logits, targets)\n",
    "        loss.backward()\n",
    "        opt_s.step()\n",
    "        total_loss += loss.item()\n",
    "    acc = evaluate(student, testloader)\n",
    "    print(f\"Epoch {epoch+1}: loss={total_loss/len(trainloader):.3f}, acc={acc:.2f}%\")\n",
    "\n",
    "torch.save(student.state_dict(), \"student_kd.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42171e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7Ô∏è‚É£ Latency Measurement\n",
    "# ----------------------------\n",
    "def measure_latency(model, n_runs=50):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    start = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = model(dummy_input)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    end = time.time()\n",
    "    return (end - start) / n_runs * 1000  # ms per inference\n",
    "\n",
    "lat_teacher = measure_latency(teacher)\n",
    "lat_student = measure_latency(student)\n",
    "\n",
    "print(f\"\\nLatency (Teacher): {lat_teacher:.2f} ms\")\n",
    "print(f\"Latency (Student): {lat_student:.2f} ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8Ô∏è‚É£ Visualization\n",
    "# ----------------------------\n",
    "plt.bar([\"Teacher\", \"Student (KD)\"], [lat_teacher, lat_student], color=[\"red\",\"green\"])\n",
    "plt.ylabel(\"Latency (ms/sample)\")\n",
    "plt.title(\"Inference Speed Comparison\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
